{"cells":[{"cell_type":"markdown","metadata":{"id":"TCxxWBZioi0N"},"source":["# Noisy Deep Q-Networks\n","\n","Noisy Networks were introduced to address exploration-exploitation challenges in reinforcement learning. The idea is to __add noise directly to the network's parameters__, allowing the model to explore actions more effectively. The Noisy Deep Q-Network (NDQN) is an extension of the Deep Q-Network (DQN) algorithm, incorporating this concept of parameterized noise for exploration.\n","\n","__Let's understend the Exploration Challenges in DQN:__\n","> In vanilla DQN, exploration is typically achieved through __epsilon-greedy strategies__, where the agent randomly explores with a probability epsilon. However, this approach __can be inefficient__, and the agent might __struggle to explore the state-action space thoroughly.__\n","\n","\n","__That's what Noisy Networks solve:__\n","> Noisy Networks, introduced by Fortunato et al. in their paper \"__Noisy Networks for Exploration__,\" propose a different approach to exploration. Instead of relying on a separate exploration strategy, __Noisy Networks inject randomness directly into the network's parameters.__\n","\n","\n","__Noisy DQN Architecture__\n","- The architecture of a Noisy DQN consists of a neural network that represents the Q-function. The key innovation is the addition of factorized Gaussian noise to the weights of the fully connected layers. The noise is parameterized and adapted during the learning process.\n","- Consider a fully connected layer with weights W and biases b. In a Noisy Network, each weight $W_i$ and bias $b_i$ is modeled as a factorized Gaussian noise:\n","  - $W_i = μ_i + σ_i * ϵ_i$\n","  - $b_i = μ'_i + σ'_i * ϵ'_i$\n","    - Here:\n","      - $μ_i$ and $μ'_i$ are the means of the weights and biases, respectively.\n","      - $σ_i$ and $σ'_i$ are the standard deviations of the weights and biases, respectively.\n","      - $ϵ_i$ and $ϵ'_i$ are random variables sampled from factorized Gaussian noise with a mean of 0 and a standard deviation of 1.\n","\n","\n","__Training Noisy DQN:__\n","\n","> During training, the network learns to __adapt the means and standard deviations of the noisy parameters__. The __stochastic nature of the noisy weights encourages exploration__ because the same input can lead to different actions due to the perturbations in the weights.\n","\n","The objective during training is to optimize the standard deviations $σ_i$ and $σ'_i$ __to control the amount of noise added to the weights(!)__\n","\n","\n","The loss function typically includes the standard Q-learning loss (e.g., mean squared error between predicted Q-values and target Q-values) and an additional term penalizing the standard deviations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJIVLLT1nYMl"},"outputs":[],"source":["!apt-get install -y xvfb\n","\n","!pip install pygame\n","!pip install gym==0.18\n","!pip install stable-baselines3\n","!pip install pytorch-lightning==1.6.0\n","!pip install pyvirtualdisplay\n","\n","!pip install git+https://github.com/GrupoTuring/PyGame-Learning-Environment\n","!pip install git+https://github.com/lusob/gym-ple"]},{"cell_type":"markdown","metadata":{"id":"ZOSJl-X7zvs4"},"source":["#### Setup virtual display"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B-Z6takfzqGk","outputId":"5afab869-76ab-4453-c424-6a386f53a913","executionInfo":{"status":"ok","timestamp":1702304032903,"user_tz":-120,"elapsed":574,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f704d7b3580>"]},"metadata":{},"execution_count":2}],"source":["from pyvirtualdisplay import Display\n","Display(visible=False, size=(1400, 900)).start()"]},{"cell_type":"markdown","metadata":{"id":"Cz8DLleGz_TF"},"source":["#### Import the necessary code libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cP5t6U7-nYoc"},"outputs":[],"source":["import copy\n","import torch\n","import random\n","import gym\n","import gym_ple\n","import matplotlib\n","\n","import numpy as np\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","\n","from collections import deque, namedtuple\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import IterableDataset\n","from torch.optim import AdamW\n","\n","from pytorch_lightning import LightningModule, Trainer\n","\n","from gym.wrappers import TransformObservation\n","\n","from stable_baselines3.common.atari_wrappers import MaxAndSkipEnv, WarpFrame\n","\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","num_gpus = torch.cuda.device_count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_IrPlU1wwPx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702304052248,"user_tz":-120,"elapsed":16,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}},"outputId":"322d3fbc-ee9f-439b-9c23-3300a0d8d459"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["# Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb#scrollTo=gKc1FNhKiVJX\n","\n","def display_video(frames, framerate=30):\n","  height, width, _ = frames[0].shape\n","  dpi = 70\n","  orig_backend = matplotlib.get_backend()\n","  matplotlib.use('Agg')\n","  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n","  matplotlib.use(orig_backend)\n","  ax.set_axis_off()\n","  ax.set_aspect('equal')\n","  ax.set_position([0, 0, 1, 1])\n","  im = ax.imshow(frames[0])\n","  def update(frame):\n","    im.set_data(frame)\n","    return [im]\n","  interval = 1000/framerate\n","  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n","                                  interval=interval, blit=True, repeat=False)\n","  return HTML(anim.to_html5_video())"]},{"cell_type":"markdown","metadata":{"id":"eLH52SgC0RRI"},"source":["#### Create the Deep Q-Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZO9u9X1CTtf"},"outputs":[],"source":["import math\n","from torch.nn.init import kaiming_uniform_, zeros_\n","\n","class NoisyLinear(nn.Module):\n","  \"\"\"\n","  A linear layer that generate noise.\n","\n","  In in NDQN, Noise Layer is a technique that encourage the agent to explore\n","  more actions in different state.\n","\n","  One of the advantages if the Noise layer, is that the parameters that\n","  generate the noise (std) are learnble and updates over the time.\n","  In other words, the DQN'll learn what the optimal noise to produce for\n","  find the right balance balance between exploration & exploitation\n","\n","  - sigma: The bigger the sigma valued, the more noisy the linear layer will be.\n","  \"\"\"\n","\n","  def __init__(self, in_features, out_features, sigma):\n","    super(NoisyLinear, self).__init__()\n","\n","    # Initial Learnable parameters for means\n","    self.w_mu = nn.Parameter(torch.empty((out_features, in_features))) # weights\n","    self.b_mu = nn.Parameter(torch.empty((out_features))) # bias\n","\n","    # Initial Learnable parameters for standard deviations (noise)\n","    self.w_sigma = nn.Parameter(torch.empty((out_features, in_features))) # weights\n","    self.b_sigma = nn.Parameter(torch.empty((out_features))) # bias\n","\n","    # Initialize learnble parameters (weights) using Kaiming uniform for means\n","    kaiming_uniform_(self.w_mu, a=math.sqrt(5))\n","    zeros_(self.b_mu)\n","\n","    # Initialize learnble parameters (weights) using Kaiming uniform\n","    # for standard deviations (noise)\n","    kaiming_uniform_(self.w_sigma, a=math.sqrt(5))\n","    zeros_(self.b_sigma)\n","\n","  def forward(self, x, sigma=0.5):\n","    \"\"\"\n","    Forward method to compute the output of the Noisy Linear layer.\n","\n","    Parameters:\n","    -----------\n","    - x: Input tensor\n","    - sigma: Standard deviation for the noise (default: 0.5)\n","\n","    Returns:\n","    - Output tensor\n","    \"\"\"\n","    if self.training:\n","      # During training, add noise to weights and biases\n","      # create normal distribution noises for wights and bias with a given std\n","      w_noise = torch.normal(0, sigma, size=self.w_mu.size()).to(device)\n","      b_noise = torch.normal(0, sigma, size=self.b_mu.size()).to(device)\n","\n","      # Compute the linear transformation with added noise\n","      return F.linear(\n","          x, # inpute tensor\n","          self.w_mu + self.w_sigma * w_noise, # Add the weigths noise\n","          self.b_mu + self.b_sigma * b_noise # Add the bias noise\n","        )\n","    else:\n","      # During evaluation, no noise is added\n","      return F.linear(x, self.W_mu, self.b_mu)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6gm8-15nYq7"},"outputs":[],"source":["class DQN(nn.Module):\n","  \"\"\"\n","  Deep noisy Q-network.\n","\n","  Returns:\n","    - the Q-values ([Q(s,a), Q(s,a), ...])\n","  \"\"\"\n","  def __init__(self, hidden_size, obs_shape, n_actions, sigma=0.5):\n","    super().__init__()\n","\n","    # Convolution network\n","    self.conv = nn.Sequential(\n","      nn.Conv2d(obs_shape[0], 64, kernel_size=3),\n","      nn.MaxPool2d(kernel_size=4),\n","      nn.ReLU(),\n","      nn.Conv2d(64, 16, kernel_size=3),\n","      nn.MaxPool2d(kernel_size=4),\n","      nn.ReLU()\n","    )\n","\n","    # Adjust the input shape from conventional layers to linear layers\n","    # (vectorize the shape to be [f, f, f, ...])\n","    conv_out_size = self._get_conv_out(obs_shape)\n","\n","    # Noisy linear layer\n","    self.head = nn.Sequential(\n","      NoisyLinear(conv_out_size, hidden_size, sigma=sigma),\n","      nn.ReLU(),\n","    )\n","\n","    # Advantage\n","    self.fc_adv = NoisyLinear(hidden_size, n_actions, sigma=sigma)\n","\n","    # Value\n","    self.fc_value = NoisyLinear(hidden_size, 1, sigma=sigma)\n","\n","\n","  def _get_conv_out(self, shape):\n","    \"\"\"\n","    Vectorize a < 1D shape into a size vector of 1D.\n","    \"\"\"\n","    conv_out = self.conv(torch.zeros(1, *shape))\n","    return int(np.prod(conv_out.size()))\n","\n","  def forward(self, x):\n","\n","    # Pass Convolution layers, then flatten the output to 1D\n","    x = self.conv(x.float()).view(x.size()[0], -1)\n","    # Pass the flatten input to the Noisy layer\n","    x = self.head(x)\n","\n","    adv = self.fc_adv(x) # Advantage\n","    value = self.fc_value(x) # Value\n","\n","    # Return [Q(s,a),Q(s,a),...]\n","    return value + adv - torch.mean(adv, dim=1, keepdim=True)"]},{"cell_type":"markdown","metadata":{"id":"bnk0wSWj0hAz"},"source":["#### Create the policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9a0b9cdnYtT"},"outputs":[],"source":["def greedy(state, net):\n","  \"\"\"\n","  A greedy policy. Choose the best action ONLY.\n","\n","  Why there is no epsilon? beacuse now, we added noise to the DQN,\n","  so we dont neet to care about balancing exploration & exploitation.\n","  \"\"\"\n","  # Make sure your state is a tensor.\n","  state = torch.tensor([state]).to(device)\n","  # Predict the [Q(s,a), Q(s,a), ...]\n","  # its represent the values of each action that interact in this state.\n","  q_values = net(state)\n","  # now, choose the BEST action for this state (the yield the highest Q(s,a))\n","  _, action = torch.max(q_values, dim=1)\n","  # Make sure the action is integer\n","  action = int(action.item())\n","  # return action.\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"brJmKGkl0jge"},"source":["#### Create the replay buffer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvHMYqlZnYvj"},"outputs":[],"source":["class ReplayBuffer:\n","  \"\"\"\n","  A Replay buffer memory that implement the Prioritized Experienc Replay technique.\n","  \"\"\"\n","\n","  def __init__(self, capacity):\n","    self.buffer = deque(maxlen=capacity)\n","    self.priorities = deque(maxlen=capacity)\n","    self.capacity = capacity\n","    self.alpha = 0.0  # anneal.\n","    self.beta = 1.0  # anneal.\n","    self.max_priority = 0.0\n","\n","  def __len__(self):\n","    return len(self.buffer)\n","\n","  def append(self, experience):\n","    self.buffer.append(experience)\n","    self.priorities.append(self.max_priority)\n","\n","  def update(self, index, priority):\n","    if priority > self.max_priority:\n","      self.max_priority = priority\n","    self.priorities[index] = priority\n","\n","  def sample(self, batch_size):\n","    prios = np.array(self.priorities, dtype=np.float64) + 1e-4 # Stability constant.\n","    prios = prios ** self.alpha\n","    probs = prios / prios.sum()\n","\n","    weights = (self.__len__() * probs) ** -self.beta\n","    weights = weights / weights.max()\n","\n","    idx = random.choices(range(self.__len__()), weights=probs, k=batch_size)\n","    sample = [(i, weights[i], *self.buffer[i]) for i in idx]\n","    return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUQcRQ4xnYyI"},"outputs":[],"source":["class RLDataset(IterableDataset):\n","  \"\"\"\n","  Iterable dataset\n","  \"\"\"\n","\n","  def __init__(self, buffer, sample_size=400):\n","    self.buffer = buffer\n","    self.sample_size = sample_size\n","\n","  def __iter__(self):\n","    for experience in self.buffer.sample(self.sample_size):\n","      yield experience"]},{"cell_type":"markdown","metadata":{"id":"0yvDC9qF0oPr"},"source":["#### Create the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2vlJCIB7bQVJ"},"outputs":[],"source":["class RunningMeanStd:\n","    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n","    def __init__(self, epsilon=1e-4, shape=()):\n","        self.mean = np.zeros(shape, \"float64\")\n","        self.var = np.ones(shape, \"float64\")\n","        self.count = epsilon\n","\n","    def update(self, x):\n","        batch_mean = np.mean(x, axis=0)\n","        batch_var = np.var(x, axis=0)\n","        batch_count = x.shape[0]\n","        self.update_from_moments(batch_mean, batch_var, batch_count)\n","\n","    def update_from_moments(self, batch_mean, batch_var, batch_count):\n","        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n","            self.mean, self.var, self.count, batch_mean, batch_var, batch_count\n","        )\n","\n","\n","def update_mean_var_count_from_moments(\n","    mean, var, count, batch_mean, batch_var, batch_count\n","):\n","    delta = batch_mean - mean\n","    tot_count = count + batch_count\n","\n","    new_mean = mean + delta * batch_count / tot_count\n","    m_a = var * count\n","    m_b = batch_var * batch_count\n","    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n","    new_var = M2 / tot_count\n","    new_count = tot_count\n","\n","    return new_mean, new_var, new_count\n","\n","\n","class NormalizeObservation(gym.core.Wrapper):\n","    def __init__(\n","        self,\n","        env,\n","        epsilon=1e-8,\n","    ):\n","        super().__init__(env)\n","        self.num_envs = getattr(env, \"num_envs\", 1)\n","        self.is_vector_env = getattr(env, \"is_vector_env\", False)\n","        if self.is_vector_env:\n","            self.obs_rms = RunningMeanStd(shape=self.single_observation_space.shape)\n","        else:\n","            self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)\n","        self.epsilon = epsilon\n","\n","    def step(self, action):\n","        obs, rews, dones, infos = self.env.step(action)\n","        if self.is_vector_env:\n","            obs = self.normalize(obs)\n","        else:\n","            obs = self.normalize(np.array([obs]))[0]\n","        return obs, rews, dones, infos\n","\n","    def reset(self, **kwargs):\n","        return_info = kwargs.get(\"return_info\", False)\n","        if return_info:\n","            obs, info = self.env.reset(**kwargs)\n","        else:\n","            obs = self.env.reset(**kwargs)\n","        if self.is_vector_env:\n","            obs = self.normalize(obs)\n","        else:\n","            obs = self.normalize(np.array([obs]))[0]\n","        if not return_info:\n","            return obs\n","        else:\n","            return obs, info\n","\n","    def normalize(self, obs):\n","        self.obs_rms.update(obs)\n","        return (obs - self.obs_rms.mean) / np.sqrt(self.obs_rms.var + self.epsilon)\n","\n","\n","class NormalizeReward(gym.core.Wrapper):\n","    def __init__(\n","        self,\n","        env,\n","        gamma=0.99,\n","        epsilon=1e-8,\n","    ):\n","        super().__init__(env)\n","        self.num_envs = getattr(env, \"num_envs\", 1)\n","        self.is_vector_env = getattr(env, \"is_vector_env\", False)\n","        self.return_rms = RunningMeanStd(shape=())\n","        self.returns = np.zeros(self.num_envs)\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","\n","    def step(self, action):\n","        obs, rews, dones, infos = self.env.step(action)\n","        if not self.is_vector_env:\n","            rews = np.array([rews])\n","        self.returns = self.returns * self.gamma + rews\n","        rews = self.normalize(rews)\n","        self.returns[dones] = 0.0\n","        if not self.is_vector_env:\n","            rews = rews[0]\n","        return obs, rews, dones, infos\n","\n","    def normalize(self, rews):\n","        self.return_rms.update(self.returns)\n","        return rews / np.sqrt(self.return_rms.var + self.epsilon)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iMjN3BoxIMNF","outputId":"9fe95059-f0c0-4c61-8959-d6b926654f8f","executionInfo":{"status":"ok","timestamp":1702304052709,"user_tz":-120,"elapsed":7,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["env = gym.make('Catcher-v0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTdRGXMHhHdj"},"outputs":[],"source":["obs = env.reset()\n","obs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V04aLOL2gjR9"},"outputs":[],"source":["env.observation_space, env.action_space"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0ppG1ncTSws"},"outputs":[],"source":["for i in range(20):\n","  obs, rew, done, info = env.step(env.action_space.sample())\n","\n","plt.imshow(obs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6po4CHxHtyI7"},"outputs":[],"source":["env = MaxAndSkipEnv(env, skip=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXMabRn9t4Mw"},"outputs":[],"source":["obs = env.reset()\n","\n","for i in range(10):\n","  obs, _, _, _ = env.step(env.action_space.sample())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7rCRq-qtyLN"},"outputs":[],"source":["type(obs), obs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4TQZfU9tyNw"},"outputs":[],"source":["plt.imshow(obs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RMGcAo7USiZK"},"outputs":[],"source":["env = WarpFrame(env, height=42, width=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRKvoPFZdrsP"},"outputs":[],"source":["obs = env.reset()\n","\n","for i in range(10):\n","  obs, _, _, _ = env.step(env.action_space.sample())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7L1nEL29druP"},"outputs":[],"source":["type(obs), obs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q3n-dMridr41"},"outputs":[],"source":["plt.imshow(obs.squeeze(), cmap='gray_r')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgnxpk1oey11"},"outputs":[],"source":["obs.min(), obs.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8E4MD8nv0TF"},"outputs":[],"source":["env = TransformObservation(env, lambda x: x.swapaxes(-1, 0))\n","env.observation_space = gym.spaces.Box(low=0, high=255, shape=(1, 42, 42), dtype=np.float32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3mWvT2Dv0Vk"},"outputs":[],"source":["obs = env.reset()\n","\n","for i in range(10):\n","  obs, _, _, _ = env.step(env.action_space.sample())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eg74mT4khdlY"},"outputs":[],"source":["obs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1t7sCsXhbHd"},"outputs":[],"source":["def create_environment(env_name):\n","  env = gym_ple.make(env_name)\n","  env = MaxAndSkipEnv(env, skip=2)\n","  env = WarpFrame(env, height=42, width=42)\n","  env = TransformObservation(env, lambda x: x.swapaxes(-1, 0))\n","  env.observation_space = gym.spaces.Box(low=0, high=255, shape=(1, 42, 42), dtype=np.float32)\n","  env = NormalizeObservation(env)\n","  env = NormalizeReward(env)\n","  return env"]},{"cell_type":"markdown","metadata":{"id":"sgXi6A4Z1p75"},"source":["#### Create the Deep Q-Learning algorithm"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"tOmxUJ1vnY5d","executionInfo":{"status":"ok","timestamp":1702309914198,"user_tz":-120,"elapsed":309,"user":{"displayName":"אור מזרחי","userId":"14772102086814831247"}}},"outputs":[],"source":["class DeepQLearning(LightningModule):\n","  \"\"\"\n","  Implements the Deep Q Learning with Prioritized Experience Replay\n","  and with Noisy DQN\n","\n","  Parameters:\n","  - env_name (str): The name of the OpenAI Gym environment.\n","  - policy (callable): The policy function for selecting actions (default: greedy).\n","  - capacity (int): The capacity of the replay buffer (default: 100,000).\n","  - batch_size (int): The batch size used for training (default: 256).\n","  - lr (float): The learning rate for the Q-network optimizer (default: 1e-3).\n","  - hidden_size (int): The size of the hidden layer in the Q-network (default: 128).\n","  - gamma (float): The discount factor for future rewards (default: 0.99).\n","  - loss_fn (callable): The loss function used for training (default: F.smooth_l1_loss).\n","  - optim (torch.optim.Optimizer): The optimizer for the Q-network (default: AdamW).\n","  - samples_per_epoch (int): The number of samples collected per epoch (default: 1,000).\n","  - sync_rate (int): The frequency at which the target Q-network is synchronized with the Q-network (default: 10).\n","  - sigma (float): The standard deviation parameter for Noisy DQN (default: 0.5).\n","  - a_start (float): The starting value of alpha (priority exponent) for prioritized experience replay (default: 0.5).\n","  - a_end (float): The ending value of alpha for prioritized experience replay (default: 0.0).\n","  - a_last_episode (int): The episode at which alpha reaches its ending value (default: 100).\n","  - b_start (float): The starting value of beta (importance sampling exponent) for prioritized experience replay (default: 0.4).\n","  - b_end (float): The ending value of beta for prioritized experience replay (default: 1.0).\n","  - b_last_episode (int): The episode at which beta reaches its ending value (default: 100).\n","  \"\"\"\n","\n","  # Initialize.\n","  def __init__(self, env_name, policy=greedy, capacity=100_000,\n","               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99,\n","               loss_fn=F.smooth_l1_loss, optim=AdamW, samples_per_epoch=1_000,\n","               sync_rate=10, sigma=0.5, a_start=0.5, a_end=0.0, a_last_episode=100,\n","               b_start=0.4, b_end=1.0, b_last_episode=100):\n","\n","    super().__init__()\n","\n","    # Create the env\n","    self.env = create_environment(env_name)\n","\n","    # Extract observation size and number of actions from the environment\n","    obs_size = self.env.observation_space.shape\n","    n_actions = self.env.action_space.n\n","\n","    # Initialize the Q-network with Noisy DQN\n","    self.q_net = DQN(hidden_size, obs_size, n_actions, sigma=sigma)\n","\n","    # Initialize the target Q-network by copying the Q-network\n","    self.target_q_net = copy.deepcopy(self.q_net)\n","\n","    # Set the action selection policy\n","    self.policy = policy\n","\n","    # Initialize the experience replay buffer\n","    self.buffer = ReplayBuffer(capacity=capacity)\n","\n","    self.save_hyperparameters()\n","\n","   # Fill the experience buffer with samples\n","    while len(self.buffer) < self.hparams.samples_per_epoch:\n","      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n","      self.play_episode()\n","\n","  @torch.no_grad()\n","  def play_episode(self, policy=None):\n","    \"\"\"\n","    Simulate an episode and collect samples to fill the experience buffer.\n","\n","    Parameters:\n","    - policy (function): Action selection policy (default: None).\n","    \"\"\"\n","    state = self.env.reset()\n","    done = False\n","\n","    while not done:\n","      if policy:\n","        action = policy(state, self.q_net)\n","      else:\n","        action = self.env.action_space.sample()\n","\n","      next_state, reward, done, info = self.env.step(action)\n","      exp = (state, action, reward, done, next_state)\n","      self.buffer.append(exp)\n","      state = next_state\n","\n","  # Forward.\n","  def forward(self, x):\n","    return self.q_net(x)\n","\n","  # Configure optimizers.\n","  def configure_optimizers(self):\n","    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n","    return [q_net_optimizer]\n","\n","  # Create dataloader.\n","  def train_dataloader(self):\n","    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n","    dataloader = DataLoader(\n","        dataset=dataset,\n","        batch_size=self.hparams.batch_size\n","    )\n","    return dataloader\n","\n","  # Training step.\n","  def training_step(self, batch, batch_idx):\n","    \"\"\"\n","    Execute a training step of a Deep Q Learning with Prioritized Experience Replay\n","    \"\"\"\n","    # Unpack the components and make sure that is in the correct format\n","    indices, weights, states, actions, rewards, dones, next_states = batch\n","    weights = weights.unsqueeze(1)\n","    actions = actions.unsqueeze(1)\n","    rewards = rewards.unsqueeze(1)\n","    dones = dones.unsqueeze(1)\n","\n","    # 1. Predict the [Q(s,a),Q(s,a),...] of the current state and actions\n","    #    by the Q-network.\n","    state_action_values = self.q_net(states).gather(1, actions)\n","\n","    # 2. Calculate the target [Q(s,a),Q(s,a),...] (for comparation and updating)\n","    with torch.no_grad():\n","      # Predict the BEST actions for the next state by the Q-network\n","      _, next_actions = self.q_net(next_states).max(dim=1, keepdim=True)\n","      # By the Target-Q-network, evaluate the [Q(s',a*), Q(s',a*), ...]\n","      next_action_values = self.target_q_net(next_states).gather(1, next_actions)\n","      # Make sure you not include any extra rewards.\n","      next_action_values[dones] = 0.0\n","\n","    # Compute the expected Q(s,a)\n","    expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n","\n","    # Compute the TD error between the actual Q(s,a) and the Target Q(s',a*)\n","    td_errors = (state_action_values - expected_state_action_values).abs().detach()\n","\n","    # Update the priorities if the experiences, based on their TD.\n","    # -> the more TD, the more potential learning.\n","    for idx, e in zip(indices, td_errors):\n","      self.buffer.update(idx, e.item())\n","\n","    # Compute the loss\n","    loss = weights * self.hparams.loss_fn(state_action_values, expected_state_action_values, reduction='none')\n","    loss = loss.mean()\n","\n","    self.log('episode/Q-Error', loss)\n","    # Return the loss for back propegation (update the DQN weights.)\n","    return loss\n","\n","  # Training epoch end.\n","  def training_epoch_end(self, training_step_outputs):\n","    \"\"\"\n","    Callback function where the training epoch is over.\n","    \"\"\"\n","    # Update the alpha & beta\n","    alpha = max(\n","        self.hparams.a_end,\n","        self.hparams.a_start - self.current_epoch / self.hparams.a_last_episode\n","    )\n","    beta = min(\n","        self.hparams.b_end,\n","        self.hparams.b_start + self.current_epoch / self.hparams.b_last_episode\n","    )\n","    # set the newer alpha & beta\n","    self.buffer.alpha = alpha\n","    self.buffer.beta = beta\n","\n","    # Play one epicode for update the memory with more newer experiences.\n","    self.play_episode(policy=self.policy)\n","    self.log('episode/Return', self.env.unwrapped.game_state.score())\n","\n","    # Update the Target netwoek weigths each `sync_rate` times.\n","    if self.current_epoch % self.hparams.sync_rate == 0:\n","      self.target_q_net.load_state_dict(self.q_net.state_dict())"]},{"cell_type":"markdown","metadata":{"id":"6mm9P0sX1wAA"},"source":["#### Purge logs and run the visualization tool (Tensorboard)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfGQdpn0nY99"},"outputs":[],"source":["!rm -r /content/lightning_logs/\n","!rm -r /content/videos/\n","%load_ext tensorboard\n","%tensorboard --logdir /content/lightning_logs/"]},{"cell_type":"markdown","metadata":{"id":"G8GdIwla1wrW"},"source":["#### Train the policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ig8c_RM8nZLN"},"outputs":[],"source":["# Create an instance of the agent\n","algo = DeepQLearning(\n","  'Catcher-v0',\n","  lr=0.0005,\n","  sigma=0.5,\n","  hidden_size=512,\n","  samples_per_epoch=1_000,\n","  a_last_episode=1_200,\n","  b_last_episode=1_200\n",")\n","\n","# Create a trainer object\n","trainer = Trainer(\n","  gpus=num_gpus,\n","  max_epochs=1_500,\n","  log_every_n_steps=1\n",")\n","\n","# training the agent\n","trainer.fit(algo)"]},{"cell_type":"markdown","metadata":{"id":"jD3x39w71xWR"},"source":["#### Check the resulting policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRwyLvCdCOO3"},"outputs":[],"source":["env = algo.env\n","policy = algo.policy\n","q_net = algo.q_net.cuda()\n","frames = []\n","\n","for episode in range(10):\n","  done = False\n","  obs = env.reset()\n","  while not done:\n","    frames.append(env.render(mode='rgb_array'))\n","    action = policy(obs, q_net)\n","    obs, _, done, _ = env.step(action)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PleQkLR-yNM"},"outputs":[],"source":["display_video(frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pX-zgOIGPlbI"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}