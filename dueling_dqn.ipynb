{"cells":[{"cell_type":"markdown","metadata":{"id":"TCxxWBZioi0N"},"source":["# Dueling Deep Q-Learning\n","\n","> Dueling Deep Q-Networks (Dueling DQN) is an extension of the Deep Q-Learning (DQN) algorithm designed to improve the learning efficiency and stability of value-based reinforcement learning agents. __The key idea behind Dueling DQN is to separate the estimation of the state value function and the advantage function, allowing the agent to better understand the value of being in a certain state and the advantage of taking a particular action within that state.__\n","\n","\n","__Basic Q-Value Decomposition:__\n","- In the standard Q-learning setting, the Q-value for a state-action pair $(s,a)$ is represented by $Q(s,a)$. This value can be decomposed into the sum of two components:\n","  - $Q(s,a) = V(s) + A(s,a)$\n","    - $V(s)$: The state value function represents the value of being in a particular state $s$ __irrespective of the action taken__. It reflects __how good it is to be in that state on average__.\n","    - $ A(s,a)$: The advantage function represents the additional value of taking a particular action $a$ in state $s$ compared to the average value of all actions in that state. It captures the benefit of selecting a specific action.\n","\n","__Dueling DQN Architecture:__\n","\n","Dueling DQN introduces a neural network architecture that explicitly models these separate components, allowing the network to estimate both the state value and the advantage for each action. The dueling architecture consists of two streams:\n","\n","1. __Value stream ($V(s)$):__\n","  - This stream estimates the state value $V(s)$ and has a single output node.\n","2. __Advantage  stream ($V(s,a)$):__\n","  - This stream estimates the advantage of each action $(A(s,a))$ and has as many output nodes as there are possible actions.\n","\n","\n","\n","__The final Q-value is obtained by combining the value and advantage streams:__\n","\n","\n","![image.png](data:image/png;base64,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)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJIVLLT1nYMl"},"outputs":[],"source":["!apt-get install -y xvfb\n","\n","!pip install pygame pytorch-lightning==1.6.0 pyvirtualdisplay\n","\n","# for the environment\n","!pip install git+https://github.com/GrupoTuring/PyGame-Learning-Environment\n","!pip install git+https://github.com/lusob/gym-ple"]},{"cell_type":"markdown","metadata":{"id":"ZOSJl-X7zvs4"},"source":["#### Setup virtual display"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-Z6takfzqGk"},"outputs":[],"source":["from pyvirtualdisplay import Display\n","Display(visible=False, size=(1400, 900)).start()"]},{"cell_type":"markdown","metadata":{"id":"Cz8DLleGz_TF"},"source":["#### Import the necessary code libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cP5t6U7-nYoc"},"outputs":[],"source":["import copy\n","import torch\n","import random\n","import gym\n","import gym_ple\n","import matplotlib\n","\n","import numpy as np\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","\n","from collections import deque, namedtuple\n","from IPython.display import HTML\n","from base64 import b64encode\n","\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import IterableDataset\n","from torch.optim import AdamW\n","\n","from pytorch_lightning import LightningModule, Trainer\n","\n","from gym.wrappers import TransformObservation\n","\n","device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n","num_gpus = torch.cuda.device_count()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z_IrPlU1wwPx"},"outputs":[],"source":["# Copied from: https://colab.research.google.com/github/deepmind/dm_control/blob/master/tutorial.ipynb#scrollTo=gKc1FNhKiVJX\n","\n","def display_video(frames, framerate=30):\n","  \"\"\"\n","  Return a HTML video format.\n","  \"\"\"\n","  height, width, _ = frames[0].shape\n","  dpi = 70\n","  orig_backend = matplotlib.get_backend()\n","  matplotlib.use('Agg')\n","  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n","  matplotlib.use(orig_backend)\n","  ax.set_axis_off()\n","  ax.set_aspect('equal')\n","  ax.set_position([0, 0, 1, 1])\n","  im = ax.imshow(frames[0])\n","  def update(frame):\n","    im.set_data(frame)\n","    return [im]\n","  interval = 1000/framerate\n","  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n","                                  interval=interval, blit=True, repeat=False)\n","  return HTML(anim.to_html5_video())"]},{"cell_type":"markdown","metadata":{"id":"eLH52SgC0RRI"},"source":["#### Create the Deep Q-Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6gm8-15nYq7"},"outputs":[],"source":["\n","class DQN(nn.Module):\n","    def __init__(self, hidden_size, obs_size, n_actions):\n","        super().__init__()\n","\n","        # Shared feature extraction network\n","        self.net = nn.Sequential(\n","            nn.Linear(obs_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU()\n","        )\n","        \"\"\"\n","        Decompose the Q(s,a) into two parts:\n","        1. The value of the state itself (fc_value).\n","          - this is the value of the state itself. (what is the avarage\n","            value if I'm in that state?).\n","          -\n","        2. The advantage of an action is that state (fc_adv).\n","          - Is the value of a specific action in that state!\n","            (its tell us if a specific action in that state is better than the\n","             avarage value of taking an action in that state.)\n","             \"there is some advantage to take this action in that state..?\"\n","        \"\"\"\n","        # 1. Value stream\n","        # - how good it is for an agent to be in that state?\n","        # - how big is the return that we expect to obtain if we follow our\n","        #   policy starting in this state?\n","        # --> Input: Features\n","        # --> Output: V(s)\n","        self.fc_value = nn.Linear(hidden_size, 1)\n","\n","        # 2. Advantage stream\n","        # - how much better or worse it is to take a specific action\n","        #   in that state over the value of that state.\n","        # --> Input: Features\n","        # --> Output: [Q(s,a),Q(s,a),...] as the number of actions.\n","        self.fc_adv = nn.Linear(hidden_size, n_actions)\n","\n","    def forward(self, x):\n","        x = self.net(x.float())   # Feature extraction - Pass the state for extraxt patterns\n","        value = self.fc_value(x)  # Estimate Value stream V(s)\n","        adv = self.fc_adv(x)      # Estimate Advantage stream [Q(s1,a1),Q(s1,a2),...]\n","\n","        # Combine value and advantage streams using dueling formula\n","        q_values = value + adv - torch.mean(adv, dim=1, keepdim=True)\n","\n","        return q_values"]},{"cell_type":"markdown","metadata":{"id":"bnk0wSWj0hAz"},"source":["#### Create the policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9a0b9cdnYtT"},"outputs":[],"source":["def epsilon_greedy(state, env, net, epsilon=0.0):\n","  \"\"\"\n","  Epsilon greedy policy. balance exploration and exploitation.\n","  \"\"\"\n","  if np.random.random() < epsilon:\n","    action = env.action_space.sample()\n","  else:\n","    state = torch.tensor([state]).to(device)\n","    q_values = net(state)\n","    _, action = torch.max(q_values, dim=1)\n","    action = int(action.item())\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"brJmKGkl0jge"},"source":["#### Create the replay buffer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvHMYqlZnYvj"},"outputs":[],"source":["class ReplayBuffer:\n","\n","  def __init__(self, capacity):\n","    self.buffer = deque(maxlen=capacity)\n","\n","  def __len__(self):\n","    return len(self.buffer)\n","\n","  def append(self, experience):\n","    self.buffer.append(experience)\n","\n","  def sample(self, batch_size):\n","    return random.sample(self.buffer, batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUQcRQ4xnYyI"},"outputs":[],"source":["class RLDataset(IterableDataset):\n","\n","  def __init__(self, buffer, sample_size=400):\n","    self.buffer = buffer\n","    self.sample_size = sample_size\n","\n","  def __iter__(self):\n","    for experience in self.buffer.sample(self.sample_size):\n","      yield experience"]},{"cell_type":"markdown","metadata":{"id":"0yvDC9qF0oPr"},"source":["#### Create the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qe1HXNCGDqIm"},"outputs":[],"source":["class RunningMeanStd:\n","  \"\"\"\n","  Class for computing the running mean and std, for keep normalize\n","  the observations and rewards in real time.\n","\n","  Normalize the observation & rewards produce better and stable results,\n","  especially use un neural networks.\n","  \"\"\"\n","    # https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n","    def __init__(self, epsilon=1e-4, shape=()):\n","        self.mean = np.zeros(shape, \"float64\")\n","        self.var = np.ones(shape, \"float64\")\n","        self.count = epsilon\n","\n","    def update(self, x):\n","        batch_mean = np.mean(x, axis=0)\n","        batch_var = np.var(x, axis=0)\n","        batch_count = x.shape[0]\n","        self.update_from_moments(batch_mean, batch_var, batch_count)\n","\n","    def update_from_moments(self, batch_mean, batch_var, batch_count):\n","        self.mean, self.var, self.count = update_mean_var_count_from_moments(\n","            self.mean, self.var, self.count, batch_mean, batch_var, batch_count\n","        )\n","\n","\n","def update_mean_var_count_from_moments(\n","    mean, var, count, batch_mean, batch_var, batch_count\n","):\n","    delta = batch_mean - mean\n","    tot_count = count + batch_count\n","\n","    new_mean = mean + delta * batch_count / tot_count\n","    m_a = var * count\n","    m_b = batch_var * batch_count\n","    M2 = m_a + m_b + np.square(delta) * count * batch_count / tot_count\n","    new_var = M2 / tot_count\n","    new_count = tot_count\n","\n","    return new_mean, new_var, new_count\n","\n","\n","class NormalizeObservation(gym.core.Wrapper):\n","  \"\"\"\n","  Normalize the observations\n","  \"\"\"\n","    def __init__(\n","        self,\n","        env,\n","        epsilon=1e-8,\n","    ):\n","        super().__init__(env)\n","        self.num_envs = getattr(env, \"num_envs\", 1)\n","        self.is_vector_env = getattr(env, \"is_vector_env\", False)\n","        if self.is_vector_env:\n","            self.obs_rms = RunningMeanStd(shape=self.single_observation_space.shape)\n","        else:\n","            self.obs_rms = RunningMeanStd(shape=self.observation_space.shape)\n","        self.epsilon = epsilon\n","\n","    def step(self, action):\n","        obs, rews, dones, infos = self.env.step(action)\n","        if self.is_vector_env:\n","            obs = self.normalize(obs)\n","        else:\n","            obs = self.normalize(np.array([obs]))[0]\n","        return obs, rews, dones, infos\n","\n","    def reset(self, **kwargs):\n","        return_info = kwargs.get(\"return_info\", False)\n","        if return_info:\n","            obs, info = self.env.reset(**kwargs)\n","        else:\n","            obs = self.env.reset(**kwargs)\n","        if self.is_vector_env:\n","            obs = self.normalize(obs)\n","        else:\n","            obs = self.normalize(np.array([obs]))[0]\n","        if not return_info:\n","            return obs\n","        else:\n","            return obs, info\n","\n","    def normalize(self, obs):\n","        self.obs_rms.update(obs)\n","        return (obs - self.obs_rms.mean) / np.sqrt(self.obs_rms.var + self.epsilon)\n","\n","\n","class NormalizeReward(gym.core.Wrapper):\n","  \"\"\"\n","  Normalize the rewards\n","  \"\"\"\n","    def __init__(\n","        self,\n","        env,\n","        gamma=0.99,\n","        epsilon=1e-8,\n","    ):\n","        super().__init__(env)\n","        self.num_envs = getattr(env, \"num_envs\", 1)\n","        self.is_vector_env = getattr(env, \"is_vector_env\", False)\n","        self.return_rms = RunningMeanStd(shape=())\n","        self.returns = np.zeros(self.num_envs)\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","\n","    def step(self, action):\n","        obs, rews, dones, infos = self.env.step(action)\n","        if not self.is_vector_env:\n","            rews = np.array([rews])\n","        self.returns = self.returns * self.gamma + rews\n","        rews = self.normalize(rews)\n","        self.returns[dones] = 0.0\n","        if not self.is_vector_env:\n","            rews = rews[0]\n","        return obs, rews, dones, infos\n","\n","    def normalize(self, rews):\n","        self.return_rms.update(self.returns)\n","        return rews / np.sqrt(self.return_rms.var + self.epsilon)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DiQoJy3vxQJ8"},"outputs":[],"source":["env = gym_ple.make(\"FlappyBird-v0\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3T5uqIaDxQMO"},"outputs":[],"source":["env.reset()\n","env.unwrapped.game_state.getGameState()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjqgTgSByhBs"},"outputs":[],"source":["list(env.unwrapped.game_state.getGameState().values())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YuR0dNuhyhDb"},"outputs":[],"source":["env.unwrapped.game_state.frame_skip = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULpe9kd9EzfH"},"outputs":[],"source":["class StateVectorWrapper(gym.Wrapper):\n","  \"\"\"\n","  Convert image observation (state) of the environment to a state vector of numbers.\n","  \"\"\"\n","\n","  def __init__(self, env):\n","    super().__init__(env)\n","    # get initial state\n","    state = self.reset()\n","    # create the observation space objects\n","    self.observation_space = gym.spaces.Box(\n","        # action space go from [-inf,inf]\n","        low=float('-inf'),\n","        high=float('inf'),\n","        # set the shape\n","        shape=state.shape\n","    )\n","\n","  def reset(self):\n","    \"\"\"\n","    Define what happens when we reset the environment\n","    \"\"\"\n","    # reset\n","    super().reset()\n","    # take the state dictionary\n","    state_dict = self.env.unwrapped.game_state.getGameState()\n","    # extract the values only (vector of numbers)\n","    state = list(state_dict.values())\n","    # return state as numpy array (vector of numbers that represent the state)\n","    return np.array(state)\n","\n","  def step(self, action):\n","    \"\"\"\n","    Define how to modify the observation when we take a step in the env\n","    \"\"\"\n","    # Perform the action\n","    _, reward, done, info = super().step(action)\n","    # return the observation, rewards, and so on (transition)\n","    next_state_dict = self.env.unwrapped.game_state.getGameState()\n","    next_state = list(next_state_dict.values())\n","    return np.array(next_state), reward, done, info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NPM3zGRAxg3m"},"outputs":[],"source":["def create_environment(name):\n","  \"\"\"\n","  Create an environment and apply the transformation that we need.\n","  \"\"\"\n","  env = gym_ple.make(name) # create env\n","  env = StateVectorWrapper(env) # Make sure the state'll return as vector\n","  env = NormalizeObservation(env) # Normalize the observations\n","  env = NormalizeReward(env) # Normalize the rewards.\n","  return env"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2kQbrqV066I"},"outputs":[],"source":["env = create_environment('FlappyBird-v0')\n","frames = []\n","\n","for episode in range(10):\n","  done = False\n","  env.reset()\n","  while not done:\n","    frames.append(env.render(mode='rgb_array'))\n","    action = env.action_space.sample()\n","    _, _, done, _ = env.step(action)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eL7ydsCHSA8T"},"outputs":[],"source":["display_video(frames)"]},{"cell_type":"markdown","metadata":{"id":"sgXi6A4Z1p75"},"source":["#### Create the Deep Q-Learning algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOmxUJ1vnY5d"},"outputs":[],"source":["class DeepQLearning(LightningModule):\n","  \"\"\"\n","  Implementing the Dueling Deep Q-learning\n","\n","  Parameters\n","  -----------\n","  - `env_name`: the name of the environment\n","  - `policy`: the policy (typically epsilon greedy)\n","  - `capacity`: size of memory\n","  - `batch_size`: batch size\n","  - `lr`: learning rate\n","  - `hidden_size`: number of hidden layers\n","  - `gamma`: discount factor for computing the expected return\n","  - `loss_fn`: loss function\n","  - `optim`: optimizer\n","  - `eps_start`: initial epsilon\n","  - `eps_end`: end value epsilon\n","  - `eps_last_episode`: last episode of updating the epsilon\n","  - `samples_per_epoch`: number of sample of generate per epoch (for pass new experiences to the memory)\n","  - `sync_rate`: each number of time to update the target ANNs.\n","  \"\"\"\n","\n","  # Initialize.\n","  def __init__(self, env_name, policy=epsilon_greedy, capacity=100_000,\n","               batch_size=256, lr=1e-3, hidden_size=128, gamma=0.99,\n","               loss_fn=F.smooth_l1_loss, optim=AdamW, eps_start=1.0, eps_end=0.15,\n","               eps_last_episode=100, samples_per_epoch=1_000, sync_rate=10):\n","\n","    super().__init__()\n","    self.env = create_environment(env_name) # create environment\n","\n","    obs_size = self.env.observation_space.shape[0] # Input shape: state\n","    n_actions = self.env.action_space.n # Output shape: actions\n","\n","    # Create the Q-network\n","    self.q_net = DQN(hidden_size, obs_size, n_actions)\n","    # Create the Target-Q-network\n","    self.target_q_net = copy.deepcopy(self.q_net)\n","    # Define the policy\n","    self.policy = policy\n","    # Define the memory\n","    self.buffer = ReplayBuffer(capacity=capacity)\n","\n","    self.save_hyperparameters()\n","\n","    # ...Generate transitions (experieces) and pass them to the memory...\n","    while len(self.buffer) < self.hparams.samples_per_epoch:\n","      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n","      self.play_episode(epsilon=self.hparams.eps_start)\n","\n","  @torch.no_grad()\n","  def play_episode(self, policy=None, epsilon=0.):\n","    \"\"\"\n","    Play one epicode.\n","\n","    Use for generate more and newer experiences for add more experiences\n","    and refresh the memoey with new transitions..\n","    \"\"\"\n","    state = self.env.reset()\n","    done = False\n","\n","    while not done:\n","      if policy:\n","        action = policy(state, self.env, self.q_net, epsilon=epsilon)\n","      else:\n","        action = self.env.action_space.sample()\n","      next_state, reward, done, info = self.env.step(action)\n","      exp = (state, action, reward, done, next_state)\n","      self.buffer.append(exp)\n","      state = next_state\n","\n","  # Forward.\n","  def forward(self, x):\n","    \"\"\"\n","    Forward propegate.\n","\n","    Return the Q values ([[Q(s,a)],[Q(s,a)],[Q(s,a)],...])\n","    \"\"\"\n","    return self.q_net(x)\n","\n","  # Configure optimizers.\n","  def configure_optimizers(self):\n","    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.lr)\n","    return [q_net_optimizer]\n","\n","  # Create dataloader.\n","  def train_dataloader(self):\n","    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n","    dataloader = DataLoader(\n","        dataset=dataset,\n","        batch_size=self.hparams.batch_size\n","    )\n","    return dataloader\n","\n","  # Training step.\n","  def training_step(self, batch, batch_idx):\n","\n","    # 1. Unpack the batch and reshape it:\n","    # [[s],[s],[s]..]\n","    # [[a],[a],[a]..]\n","    # ...\n","    # [[s'],[s'],[s']..]\n","    states, actions, rewards, dones, next_states = batch\n","    actions = actions.unsqueeze(1)\n","    rewards = rewards.unsqueeze(1)\n","    dones = dones.unsqueeze(1)\n","\n","    # 2. Given current states and actions, compute Q(s,a)\n","    # will return [Q(s,a),Q(s,a),Q(s,a),....]\n","    state_action_values = self.q_net(states).gather(1, actions)\n","\n","    # 3. Update the Q-netwoek using the target.\n","    with torch.no_grad():\n","      # For the NEXT states, predict by the Q-network the max Q-values\n","      # that came from the best actions!\n","      # returns the best actions to perform in the next states: [a*, a*, ...]\n","      _, next_actions = self.q_net(next_states).max(dim=1, keepdim=True)\n","      # Take the predicted best actions for the next state, and evaluate\n","      # the next states & best actions by the Target-Q-network.\n","      # it'll reutrn the values of: [Q(s',a*), Q(s',a*), ...]\n","      next_action_values = self.target_q_net(next_states).gather(1, next_actions)\n","      # Make sure that you don't calculate any extra rewards.\n","      next_action_values[dones] = 0.0\n","\n","    # 4. Compute the expected Q(s,a) -> the target\n","    expected_state_action_values = rewards + self.hparams.gamma * next_action_values\n","\n","    # 5. Compute the loss between the actual Q(s,a) and the target Q(s',a*).\n","    loss = self.hparams.loss_fn(state_action_values, expected_state_action_values)\n","    self.log('episode/Q-Error', loss)\n","    return loss\n","\n","  # Training epoch end.\n","  def training_epoch_end(self, training_step_outputs):\n","    \"\"\"\n","    Callback function where a training epoch done.\n","    \"\"\"\n","    # compute the epsilon\n","    epsilon = max(\n","        self.hparams.eps_end,\n","        self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n","    )\n","\n","    # play an epicode for gain more experience, and pass more experiences\n","    # to the memory with the most resent policy.\n","    self.play_episode(policy=self.policy, epsilon=epsilon)\n","    self.log('episode/Return', self.env.unwrapped.game_state.score())\n","\n","    # Update the Target-Q-network weigths every `sync_rate` epochs\n","    if self.current_epoch % self.hparams.sync_rate == 0:\n","      self.target_q_net.load_state_dict(self.q_net.state_dict())"]},{"cell_type":"markdown","metadata":{"id":"6mm9P0sX1wAA"},"source":["#### Purge logs and run the visualization tool (Tensorboard)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfGQdpn0nY99"},"outputs":[],"source":["!rm -r /content/lightning_logs/\n","!rm -r /content/videos/\n","%load_ext tensorboard\n","%tensorboard --logdir /content/lightning_logs/"]},{"cell_type":"markdown","metadata":{"id":"G8GdIwla1wrW"},"source":["#### Train the policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ig8c_RM8nZLN"},"outputs":[],"source":["algo = DeepQLearning(\n","  'FlappyBird-v0',\n","  lr=5e-4,\n","  hidden_size=512,\n","  eps_end=0.01,\n","  eps_last_episode=1_000,\n","  capacity=10_000,\n","  gamma=0.9\n",")\n","\n","trainer = Trainer(\n","  gpus=num_gpus,\n","  max_epochs=3_000,\n","  log_every_n_steps=1\n",")\n","\n","trainer.fit(algo)"]},{"cell_type":"markdown","metadata":{"id":"jD3x39w71xWR"},"source":["#### Check the resulting policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PleQkLR-yNM"},"outputs":[],"source":["env = algo.env\n","policy = algo.policy\n","q_net = algo.q_net\n","frames = []\n","\n","for episode in range(10):\n","  done = False\n","  obs = env.reset()\n","  while not done:\n","    frames.append(env.render(mode='rgb_array'))\n","    action = policy(obs, env, q_net)\n","    obs, _, done, _ = env.step(action)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRwyLvCdCOO3"},"outputs":[],"source":["display_video(frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLwhTqDNiupZ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}